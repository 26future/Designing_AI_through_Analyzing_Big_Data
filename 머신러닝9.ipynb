{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  #자연어(영어) 처리 도구\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download() #nltk data 다운로드\n",
    "#nltk.download('treebank') #treebank 별도로 설치\n",
    "#github.com/nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install konlpy\n",
    "import konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JPype: 자바와 파이썬 연결\n",
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "print(kkma.morphs('자바와 파이썬 연결'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#토큰화, 정제, 정규화\n",
    "#토큰(단어)화: 어절,단어,문장,문단이 될 수도 있음\n",
    "#단어 토큰화: 구두점,특수문자 제거\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#목적에 맞는 tokenizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Jone', \"'s\", 'Do', \"n't\", 'be']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(\"Mr. Jone's Don't be \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr', '.', 'Jone', \"'\", 's', 'Don', \"'\", 't', 'be']\n"
     ]
    }
   ],
   "source": [
    "print(WordPunctTokenizer().tokenize(\"Mr. Jone's Don't be \")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mr', \"jone's\", \"don't\", 'be']\n"
     ]
    }
   ],
   "source": [
    "print(text_to_word_sequence(\"Mr. Jone's Don't be!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정규표현식을 잘 써야 전처리 => 양질의 데이터를 얻을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Jone', \"'s\", 'Do', \"n't\", 'be', '!']\n"
     ]
    }
   ],
   "source": [
    "twt=TreebankWordTokenizer()\n",
    "print(twt.tokenize(\"Mr. Jone's Don't be!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python is an interpreted, high-level, general-purpose programming language.', \"Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\", 'Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text)) #token: 3개 => 토큰의 단위가 문장\n",
    "print(len(sent_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#조사: 그녀가(에게,를,와,는...)\n",
    "#아버지가방에 들어가...\n",
    "#한국어 토큰화 작업 : 형태소(가장 작은 말의 단위) 고려\n",
    "#자립형태소(길동이,파이썬)/의존형태소(조사,어미.../가,을,합니다)\n",
    "#ex)길동이가 파이썬을 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'is', 'an', 'interpreted', ',', 'high-level', ',', 'general-purpose', 'programming', 'language', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Python', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('interpreted', 'JJ'),\n",
       " (',', ','),\n",
       " ('high-level', 'JJ'),\n",
       " (',', ','),\n",
       " ('general-purpose', 'JJ'),\n",
       " ('programming', 'NN'),\n",
       " ('language', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#토큰화 과정에서 단어가 어떤 품사로 쓰였는지 구분하는 작업\n",
    "#품사 태깅(tagging), fly\n",
    "text = \"Python is an interpreted, high-level, general-purpose programming language.\"\n",
    "print(word_tokenize(text))\n",
    "from nltk.tag import pos_tag\n",
    "pos_tag(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#형태소 분석기: Okt, Mecab, Kkma...\n",
    "#단어토큰화가 아님. 형태소 단위로 토큰화를 하는 것 => 한국어\n",
    "#단어토큰화 => 영어\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '은', '수요일', ',', '내일', '은', '목요일', '입니다']\n",
      "[('오늘', 'Noun'), ('은', 'Josa'), ('수요일', 'Noun'), (',', 'Punctuation'), ('내일', 'Noun'), ('은', 'Josa'), ('목요일', 'Noun'), ('입니다', 'Adjective')]\n",
      "['오늘', '수요일', '내일', '목요일']\n"
     ]
    }
   ],
   "source": [
    "#Okt와 jpype버전이 맞지 않는 경우 에러가 발생\n",
    "#jpype1 버전을 0.7.0으로 재설치\n",
    "#pipe install jpype1==0.7.0\n",
    "okt=Okt()\n",
    "#mortphs: 형태소 추출\n",
    "print(okt.morphs(\"오늘은 수요일, 내일은 목요일입니다\"))\n",
    "#pos: 품사 추출\n",
    "print(okt.pos(\"오늘은 수요일, 내일은 목요일입니다\"))\n",
    "#nouns: 명사 추출\n",
    "print(okt.nouns(\"오늘은 수요일, 내일은 목요일입니다\"))\n",
    "#konlpy.org 사이트 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from konlpy.tag import Kkma\n",
    "# kma = Kkma()\n",
    "# kma.morphs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 정제(전처리): 불필요한 형태소 제거, 단어 통일(밥,식사...)\n",
    "#정규표현식을 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 특이값 분해 full SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[0,0,0,1,0,1,1,0,0],\n",
    "              [0,0,0,1,1,0,1,0,0],\n",
    "              [0,1,1,0,2,0,0,0,0],\n",
    "              [1,0,0,0,0,0,0,1,1]])\n",
    "np.shape(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,s,VT = np.linalg.svd(A,full_matrices=True) #full SVD, truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.24,  0.75,  0.  , -0.62],\n",
       "       [-0.51,  0.44, -0.  ,  0.74],\n",
       "       [-0.83, -0.49, -0.  , -0.27],\n",
       "       [-0.  , -0.  ,  1.  ,  0.  ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.round(2) #직교행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.69 2.05 1.73 0.77]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(s.round(2)) #4\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "S = np.zeros((4,9))\n",
    "S[:4,:4] = np.diag(s)\n",
    "print(S.round(2)) #특이값 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]\n",
      " [ 0.58 -0.    0.    0.   -0.    0.   -0.    0.58  0.58]\n",
      " [ 0.   -0.35 -0.35  0.16  0.25 -0.8   0.16 -0.   -0.  ]\n",
      " [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]\n",
      " [-0.29  0.31 -0.78 -0.24  0.23  0.23  0.01  0.14  0.14]\n",
      " [-0.29 -0.1   0.26 -0.59 -0.08 -0.08  0.66  0.14  0.14]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19 -0.25  0.75]]\n"
     ]
    }
   ],
   "source": [
    "print(VT.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  1., -0.,  1.,  1.,  0.,  0.],\n",
       "       [ 0., -0., -0.,  1.,  1., -0.,  1., -0., -0.],\n",
       "       [ 0.,  1.,  1., -0.,  2., -0., -0.,  0.,  0.],\n",
       "       [ 1., -0.,  0.,  0., -0.,  0., -0.,  1.,  1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(np.dot(U,S),VT).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(A,np.dot(np.dot(U,S),VT).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.69 0.  ]\n",
      " [0.   2.05]]\n"
     ]
    }
   ],
   "source": [
    "##절단된(truncated) SVD\n",
    "##토픽 2개\n",
    "S = S[:2,:2]\n",
    "print(S.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.24,  0.75],\n",
       "       [-0.51,  0.44],\n",
       "       [-0.83, -0.49],\n",
       "       [-0.  , -0.  ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape #4,4\n",
    "U = U[:,:2] #4,2 #문서4개, 토픽2\n",
    "#U의 의미는 4개 문서 각각에 대한 잠재된 의미를 표현하고 있는 수치 문서 벡터\n",
    "U.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.  , -0.31, -0.31, -0.28, -0.8 , -0.09, -0.28, -0.  , -0.  ],\n",
       "       [ 0.  , -0.24, -0.24,  0.58, -0.26,  0.37,  0.58, -0.  , -0.  ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VT.shape #(9,9)\n",
    "VT = VT[:2,:] #(2,9) #토픽*단어 개수\n",
    "#VT의 각 열은 잠재 의미를 나타내기 위한 수치화된 단어 벡터\n",
    "VT.round(2)\n",
    "#U*S*VT != A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]\n",
      " [ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]\n",
      " [ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.   -0.    0.    0.    0.  ]]\n",
      "[[0 0 0 1 0 1 1 0 0]\n",
      " [0 0 0 1 1 0 1 0 0]\n",
      " [0 1 1 0 2 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "Aprime = np.dot(np.dot(U,S),VT)\n",
    "print(Aprime.round(2))\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.  , -0.83, -0.83, -0.75, -2.16, -0.24, -0.75, -0.  , -0.  ],\n",
       "       [ 0.  , -0.49, -0.49,  1.2 , -0.53,  0.75,  1.2 , -0.  , -0.  ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(S,VT).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코사인 유사도로 유사한 문서 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "           돈      사랑      저는      좋아요\n",
    "줄거리1    0        1          1          1\n",
    "줄거리2    1        0          1          1\n",
    "줄거리3    2        0          2          2\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mov1 = np.array([0,1,1,1])\n",
    "mov2 = np.array([1,0,1,1])\n",
    "mov3 = np.array([2,0,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(mov1,mov2) #두 벡터의 내적\n",
    "np.dot(mov1,mov2)/(norm(mov1)*norm(mov2))\n",
    "\n",
    "def cos_sim(X,Y):\n",
    "    return np.dot(X,Y)/(norm(X)*norm(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666667\n",
      "1.0000000000000002\n",
      "0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim(mov1,mov2))\n",
    "print(cos_sim(mov2,mov3))\n",
    "print(cos_sim(mov1,mov3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45466, 24)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"movies_metadata.csv/movies_metadata.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.head(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['overview'].isnull().sum()\n",
    "data['overview'] = data['overview'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['overview'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 47487)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_mat = tfidf.fit_transform(data['overview'])\n",
    "tfidf_mat.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
