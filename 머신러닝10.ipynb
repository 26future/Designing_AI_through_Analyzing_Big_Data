{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "표제어(Lemmatization) 추출: 서로 형태는 다르지만, root 단어를 가지로 비교해서, \n",
    "                            전체적으로 단어의 개수를 줄이자\n",
    "                            am, are, is, was, were... => be(표제어)\n",
    "                            \n",
    "형태소: stem(어간: 단어의 의미), affix(접사: 부가적 의미)\n",
    "형태소 파싱: 어간, 접사를 분리하는 작업\n",
    "dog(독립형태소)\n",
    "dogs=dog(어간)+s(접사)\n",
    "\n",
    "WordNetLemmatizer: NLTK에 표제어 추출 도구\n",
    "\"\"\"\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wnl.lemmatize('watched') #watched\n",
    "wnl.lemmatize('watched','v')\n",
    "wnl.lemmatize('has','v')\n",
    "wnl.lemmatize('dies','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#어간 추출\n",
    "text = \"Python is an interpreted, high-level, general-purpose programming language.\"\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'is', 'an', 'interpreted', ',', 'high-level', ',', 'general-purpose', 'programming', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'is', 'an', 'interpret', ',', 'high-level', ',', 'general-purpos', 'program', 'languag', '.']\n"
     ]
    }
   ],
   "source": [
    "print([ps.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electric\n",
      "formal\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem(\"electricical\"))\n",
    "print(ps.stem(\"formalize\"))\n",
    "#구글: 마틴 포터 or 토퍼스태머 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.stem('going') #go\n",
    "ps.stem('gone') #gone\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = LancasterStemmer()\n",
    "ls.stem('going') #going\n",
    "ls.stem('gone') #gon\n",
    "ls.stem('dies') #die"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어:stopwords\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "sw = stopwords.words('english')\n",
    "ex = \"Family is not an important thing. It's everything.\"\n",
    "wt = word_tokenize(ex)\n",
    "res=[]\n",
    "for w in wt:\n",
    "    if w not in sw: #stopwords가 아니라면\n",
    "        res.append(w)\n",
    "print(wt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['최근', '코로나19로', '인한', '확진자', '및', '사망자가', '증가하고', '있습니다', '.', '코로나19를', '이겨냅시다']\n",
      "['코로나19로', '확진자', '사망자가', '증가하고', '있습니다', '.', '코로나19를', '이겨냅시다']\n"
     ]
    }
   ],
   "source": [
    "# www.ranks.nl/stopwords/korean\n",
    "#stopwords를 직접 지정하여 제거\n",
    "\n",
    "ex=\"\"\"\n",
    "최근 코로나19로 인한 확진자 및 사망자가 증가하고 있습니다. 코로나19를 이겨냅시다\n",
    "\"\"\"\n",
    "\n",
    "stop_words = \"인한 증가 최근 및\"\n",
    "stop_words = stop_words.split(\" \")\n",
    "wt = word_tokenize(ex) #공백문자로 분리\n",
    "print(wt)\n",
    "res=[]\n",
    "for w in wt:\n",
    "    if w not in stop_words:\n",
    "        res.append(w)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python is an interpreted, high-level, general-purpose programming language.',\n",
       " \"Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\",\n",
       " 'Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import * #모든 함수 import\n",
    "text = \"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\"\n",
    "text = sent_tokenize(text) #문장 단위로 tokenize #3개 문장\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['python', 'interpreted', 'high-level', 'general-purpose', 'programming', 'language'], ['created', 'guido', 'van', 'rossum', 'first', 'released', '1991', 'python', 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'notable', 'use', 'significant', 'whitespace'], ['language', 'constructs', 'object-oriented', 'approach', 'aim', 'help', 'programmers', 'write', 'clear', 'logical', 'code', 'small', 'large-scale', 'projects']]\n"
     ]
    }
   ],
   "source": [
    "#모든 단어를 소문자, 불용어 제거, 길이가 2이하 제거\n",
    "# print(sw)\n",
    "\n",
    "voc={}\n",
    "sentences=[]\n",
    "for t in text:\n",
    "    words = word_tokenize(t)\n",
    "    res=[]\n",
    "    for word in words:\n",
    "        word = word.lower() #소문자 변환\n",
    "        if word not in sw: #불용어 제거\n",
    "            if len(word) > 2: #길이가 2이상인 단어\n",
    "                res.append(word)\n",
    "                if word not in voc: #word가 voc에 없으면 키 값을 생성,초기화\n",
    "                    voc[word]=0\n",
    "                voc[word]+=1 #word가 voc에 있으면 값에 1을 더해줌\n",
    "    sentences.append(res)\n",
    "                \n",
    "# print(res)\n",
    "# print(voc)\n",
    "print(sentences) #[문단[문장1,문장2,문장3]]\n",
    "#voc={'python':3, ...}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('python', 2),\n",
       " ('language', 2),\n",
       " ('code', 2),\n",
       " ('interpreted', 1),\n",
       " ('high-level', 1),\n",
       " ('general-purpose', 1),\n",
       " ('programming', 1),\n",
       " ('created', 1),\n",
       " ('guido', 1),\n",
       " ('van', 1),\n",
       " ('rossum', 1),\n",
       " ('first', 1),\n",
       " ('released', 1),\n",
       " ('1991', 1),\n",
       " ('design', 1),\n",
       " ('philosophy', 1),\n",
       " ('emphasizes', 1),\n",
       " ('readability', 1),\n",
       " ('notable', 1),\n",
       " ('use', 1),\n",
       " ('significant', 1),\n",
       " ('whitespace', 1),\n",
       " ('constructs', 1),\n",
       " ('object-oriented', 1),\n",
       " ('approach', 1),\n",
       " ('aim', 1),\n",
       " ('help', 1),\n",
       " ('programmers', 1),\n",
       " ('write', 1),\n",
       " ('clear', 1),\n",
       " ('logical', 1),\n",
       " ('small', 1),\n",
       " ('large-scale', 1),\n",
       " ('projects', 1)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs = sorted(voc.items(), key=lambda x:x[1], reverse=True) #튜플로 묶어서 리스트 구성 => 정렬\n",
    "vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python': 1, 'language': 2, 'code': 3}\n"
     ]
    }
   ],
   "source": [
    "wi={}\n",
    "i=0\n",
    "for w,f in vs:\n",
    "    if f > 1: #언급된 빈도수가 최소 2이상인 경우\n",
    "        i+=1\n",
    "        wi[w] = i #index 부여\n",
    "print(wi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "language\n",
      "code\n"
     ]
    }
   ],
   "source": [
    "a = wi.items()\n",
    "for w,i in a:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['code']\n"
     ]
    }
   ],
   "source": [
    "vocSize=2 #가장 많이 언급된 2개의 단어만 추출\n",
    "\n",
    "#단어의 인덱스가 vocSize를 초과하는 단어 추출\n",
    "wordFreq = [w for w,i in wi.items() if i>vocSize]\n",
    "print(wordFreq)\n",
    "for w in wordFreq:\n",
    "    del wi[w]\n",
    "#인덱스(index)가 3번 이상인 단어는 제거(1,2번만 추출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python': 1, 'language': 2}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OOV(Out of Vocabulary: 단어집합에 없는 단어, 챗봇이 처음 보는 단어)\n",
    "\"\"\"\n",
    "영수:철수야 안녕? (입력 데이터, x)\n",
    "철수:응 너도 안녕. (출력 데이터, y)\n",
    "...\n",
    "철수야 안녕? -> 모델 -> 응 너도 안녕.\n",
    "\"\"\"\n",
    "#개체명 인식: 이름을 이름이라고 인식하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['python',\n",
       "  'interpreted',\n",
       "  'high-level',\n",
       "  'general-purpose',\n",
       "  'programming',\n",
       "  'language'],\n",
       " ['created',\n",
       "  'guido',\n",
       "  'van',\n",
       "  'rossum',\n",
       "  'first',\n",
       "  'released',\n",
       "  '1991',\n",
       "  'python',\n",
       "  'design',\n",
       "  'philosophy',\n",
       "  'emphasizes',\n",
       "  'code',\n",
       "  'readability',\n",
       "  'notable',\n",
       "  'use',\n",
       "  'significant',\n",
       "  'whitespace'],\n",
       " ['language',\n",
       "  'constructs',\n",
       "  'object-oriented',\n",
       "  'approach',\n",
       "  'aim',\n",
       "  'help',\n",
       "  'programmers',\n",
       "  'write',\n",
       "  'clear',\n",
       "  'logical',\n",
       "  'code',\n",
       "  'small',\n",
       "  'large-scale',\n",
       "  'projects']]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#원핫인코딩\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "tok = okt.morphs(\"나는 자연어 처리를 학습한다\")\n",
    "#원핫벡터: 단어 집합을 벡터로 표현하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '학습': 5, '한다': 6}\n"
     ]
    }
   ],
   "source": [
    "w2i={}\n",
    "for v in tok:\n",
    "    if v not in w2i.keys():\n",
    "        w2i[v] = len(w2i)\n",
    "print(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#'자연어' -> 원핫 -> 0010000\n",
    "def ohe(w,w2i):\n",
    "    ohv = [0]*len(w2i)\n",
    "    index = w2i[w]\n",
    "    ohv[index] = 1\n",
    "    return ohv\n",
    "    \n",
    "print(ohe(\"자연어\",w2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0]*len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'판다스': 1, '데이터': 2, '분석은': 3, '최고야': 4, '곰이야': 5}\n"
     ]
    }
   ],
   "source": [
    "#케라스 원핫인코딩: to_categorical()\n",
    "text = \"데이터 분석은 판다스 최고야 판다스 곰이야\"\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts([text])\n",
    "print(tok.word_index)\n",
    "#단어집합(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"판다스 분석은 동물원에서 한다\"\n",
    "enc = tok.texts_to_sequences([sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어 분리(BPE) => 기계번역\n",
    "#학습과정에서 사용되지 않은 단어가 테스트과정에서\n",
    "#입력되면 -> OOV문제 -> 제대로 모델이 동작X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run-length 기법  aaaabbbaaaaa =>a4b3a5\n",
    "# 허프만 트리(인코딩)를 이용한 압축\n",
    "# a=>101, b=>10, c=>1101 ...\n",
    "# BPE 알고리즘 => 단어 분리에 응용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AAABDAAABAC\n",
    "# BPE 압축\n",
    "# 연속적인 글자 쌍(2글자)을 구성했을때, 가장 많이 등장\n",
    "# 1) AA가 가장 많이 등장 => 다른 글자로 치환\n",
    "# => 소문자 z로 치환\n",
    "# zABDzABAC\n",
    "\n",
    "# 2) AB가 가장 많이 등장 => 다른 글자로 치환\n",
    "# => 소문자 y로 치환\n",
    "# zyDzyAC\n",
    "\n",
    "# 3) zy가 가장 많이 등장 => 다른 글자로 치환\n",
    "# => 소문자 x로 치환\n",
    "# xDxAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "###토픽 모델링 => LSA\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=(\"header\",\"footers\",\"quotes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = dataset.data\n",
    "len(documents)  #11314건의 뉴스기사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: ab4z@Virginia.EDU (\"Andi Beyer\")\\nSubject: Re: Israeli Terrorism\\nOrganization: University of Virginia\\nLines: 15'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(documents)\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.. _20newsgroups_dataset:\\n\\nThe 20 newsgroups text dataset\\n------------------------------\\n\\nThe 20 newsgroups dataset comprises around 18000 newsgroups posts on\\n20 topics split in two subsets: one for training (or development)\\nand the other one for testing (or for performance evaluation). The split\\nbetween the train and test set is based upon a messages posted before\\nand after a specific date.\\n\\nThis module contains two loaders. The first one,\\n:func:`sklearn.datasets.fetch_20newsgroups`,\\nreturns a list of the raw texts that can be fed to text feature\\nextractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\\nwith custom parameters so as to extract feature vectors.\\nThe second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\\nreturns ready-to-use features, i.e., it is not necessary to use a feature\\nextractor.\\n\\n**Data Set Characteristics:**\\n\\n    =================   ==========\\n    Classes                     20\\n    Samples total            18846\\n    Dimensionality               1\\n    Features                  text\\n    =================   ==========\\n\\nUsage\\n~~~~~\\n\\nThe :func:`sklearn.datasets.fetch_20newsgroups` function is a data\\nfetching / caching functions that downloads the data archive from\\nthe original `20 newsgroups website`_, extracts the archive contents\\nin the ``~/scikit_learn_data/20news_home`` folder and calls the\\n:func:`sklearn.datasets.load_files` on either the training or\\ntesting set folder, or both of them::\\n\\n  >>> from sklearn.datasets import fetch_20newsgroups\\n  >>> newsgroups_train = fetch_20newsgroups(subset=\\'train\\')\\n\\n  >>> from pprint import pprint\\n  >>> pprint(list(newsgroups_train.target_names))\\n  [\\'alt.atheism\\',\\n   \\'comp.graphics\\',\\n   \\'comp.os.ms-windows.misc\\',\\n   \\'comp.sys.ibm.pc.hardware\\',\\n   \\'comp.sys.mac.hardware\\',\\n   \\'comp.windows.x\\',\\n   \\'misc.forsale\\',\\n   \\'rec.autos\\',\\n   \\'rec.motorcycles\\',\\n   \\'rec.sport.baseball\\',\\n   \\'rec.sport.hockey\\',\\n   \\'sci.crypt\\',\\n   \\'sci.electronics\\',\\n   \\'sci.med\\',\\n   \\'sci.space\\',\\n   \\'soc.religion.christian\\',\\n   \\'talk.politics.guns\\',\\n   \\'talk.politics.mideast\\',\\n   \\'talk.politics.misc\\',\\n   \\'talk.religion.misc\\']\\n\\nThe real data lies in the ``filenames`` and ``target`` attributes. The target\\nattribute is the integer index of the category::\\n\\n  >>> newsgroups_train.filenames.shape\\n  (11314,)\\n  >>> newsgroups_train.target.shape\\n  (11314,)\\n  >>> newsgroups_train.target[:10]\\n  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\\n\\nIt is possible to load only a sub-selection of the categories by passing the\\nlist of the categories to load to the\\n:func:`sklearn.datasets.fetch_20newsgroups` function::\\n\\n  >>> cats = [\\'alt.atheism\\', \\'sci.space\\']\\n  >>> newsgroups_train = fetch_20newsgroups(subset=\\'train\\', categories=cats)\\n\\n  >>> list(newsgroups_train.target_names)\\n  [\\'alt.atheism\\', \\'sci.space\\']\\n  >>> newsgroups_train.filenames.shape\\n  (1073,)\\n  >>> newsgroups_train.target.shape\\n  (1073,)\\n  >>> newsgroups_train.target[:10]\\n  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\\n\\nConverting text to vectors\\n~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\nIn order to feed predictive or clustering models with the text data,\\none first need to turn the text into vectors of numerical values suitable\\nfor statistical analysis. This can be achieved with the utilities of the\\n``sklearn.feature_extraction.text`` as demonstrated in the following\\nexample that extract `TF-IDF`_ vectors of unigram tokens\\nfrom a subset of 20news::\\n\\n  >>> from sklearn.feature_extraction.text import TfidfVectorizer\\n  >>> categories = [\\'alt.atheism\\', \\'talk.religion.misc\\',\\n  ...               \\'comp.graphics\\', \\'sci.space\\']\\n  >>> newsgroups_train = fetch_20newsgroups(subset=\\'train\\',\\n  ...                                       categories=categories)\\n  >>> vectorizer = TfidfVectorizer()\\n  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\\n  >>> vectors.shape\\n  (2034, 34118)\\n\\nThe extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\\ncomponents by sample in a more than 30000-dimensional space\\n(less than .5% non-zero features)::\\n\\n  >>> vectors.nnz / float(vectors.shape[0])       # doctest: +ELLIPSIS\\n  159.01327...\\n\\n:func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \\nreturns ready-to-use token counts features instead of file names.\\n\\n.. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\\n.. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\\n\\n\\nFiltering text for more realistic training\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\nIt is easy for a classifier to overfit on particular things that appear in the\\n20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\\nhigh F-scores, but their results would not generalize to other documents that\\naren\\'t from this window of time.\\n\\nFor example, let\\'s look at the results of a multinomial Naive Bayes classifier,\\nwhich is fast to train and achieves a decent F-score::\\n\\n  >>> from sklearn.naive_bayes import MultinomialNB\\n  >>> from sklearn import metrics\\n  >>> newsgroups_test = fetch_20newsgroups(subset=\\'test\\',\\n  ...                                      categories=categories)\\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\\n  >>> clf = MultinomialNB(alpha=.01)\\n  >>> clf.fit(vectors, newsgroups_train.target)\\n  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\\n\\n  >>> pred = clf.predict(vectors_test)\\n  >>> metrics.f1_score(newsgroups_test.target, pred, average=\\'macro\\')  # doctest: +ELLIPSIS\\n  0.88213...\\n\\n(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\\nthe training and test data, instead of segmenting by time, and in that case\\nmultinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\\nyet of what\\'s going on inside this classifier?)\\n\\nLet\\'s take a look at what the most informative features are:\\n\\n  >>> import numpy as np\\n  >>> def show_top10(classifier, vectorizer, categories):\\n  ...     feature_names = np.asarray(vectorizer.get_feature_names())\\n  ...     for i, category in enumerate(categories):\\n  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\\n  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\\n  ...\\n  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\\n  alt.atheism: edu it and in you that is of to the\\n  comp.graphics: edu in graphics it is for and of to the\\n  sci.space: edu it that is in and space to of the\\n  talk.religion.misc: not it you in is that and to of the\\n\\n\\nYou can now see many things that these features have overfit to:\\n\\n- Almost every group is distinguished by whether headers such as\\n  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\\n- Another significant feature involves whether the sender is affiliated with\\n  a university, as indicated either by their headers or their signature.\\n- The word \"article\" is a significant feature, based on how often people quote\\n  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\\n  wrote:\"\\n- Other features match the names and e-mail addresses of particular people who\\n  were posting at the time.\\n\\nWith such an abundance of clues that distinguish newsgroups, the classifiers\\nbarely have to identify topics from text at all, and they all perform at the\\nsame high level.\\n\\nFor this reason, the functions that load 20 Newsgroups data provide a\\nparameter called **remove**, telling it what kinds of information to strip out\\nof each file. **remove** should be a tuple containing any subset of\\n``(\\'headers\\', \\'footers\\', \\'quotes\\')``, telling it to remove headers, signature\\nblocks, and quotation blocks respectively.\\n\\n  >>> newsgroups_test = fetch_20newsgroups(subset=\\'test\\',\\n  ...                                      remove=(\\'headers\\', \\'footers\\', \\'quotes\\'),\\n  ...                                      categories=categories)\\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\\n  >>> pred = clf.predict(vectors_test)\\n  >>> metrics.f1_score(pred, newsgroups_test.target, average=\\'macro\\')  # doctest: +ELLIPSIS\\n  0.77310...\\n\\nThis classifier lost over a lot of its F-score, just because we removed\\nmetadata that has little to do with topic classification.\\nIt loses even more if we also strip this metadata from the training data:\\n\\n  >>> newsgroups_train = fetch_20newsgroups(subset=\\'train\\',\\n  ...                                       remove=(\\'headers\\', \\'footers\\', \\'quotes\\'),\\n  ...                                       categories=categories)\\n  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\\n  >>> clf = MultinomialNB(alpha=.01)\\n  >>> clf.fit(vectors, newsgroups_train.target)\\n  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\\n\\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\\n  >>> pred = clf.predict(vectors_test)\\n  >>> metrics.f1_score(newsgroups_test.target, pred, average=\\'macro\\')  # doctest: +ELLIPSIS\\n  0.76995...\\n\\nSome other classifiers cope better with this harder version of the task. Try\\nrunning :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\\nthe ``--filter`` option to compare the results.\\n\\n.. topic:: Recommendation\\n\\n  When evaluating text classifiers on the 20 Newsgroups data, you\\n  should strip newsgroup-related metadata. In scikit-learn, you can do this by\\n  setting ``remove=(\\'headers\\', \\'footers\\', \\'quotes\\')``. The F-score will be\\n  lower because it is more realistic.\\n\\n.. topic:: Examples\\n\\n   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\\n\\n   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\\n'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names\n",
    "dataset.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: timmbake@mcl.ucsb.edu (Bake Timmons)\\nSubject: Re: Amusing atheists and agnostics\\nLines: 66\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 토픽과 가장 관련성이 높은 단어를 10개씩 출력\n",
    "# topic1: \n",
    "# ~\n",
    "# topic20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>From: ab4z@Virginia.EDU (\"Andi Beyer\")\\nSubjec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>From: timmbake@mcl.ucsb.edu (Bake Timmons)\\nSu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>From: bc744@cleveland.Freenet.Edu (Mark Ira Ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>From: ray@ole.cdac.com (Ray Berry)\\nSubject: C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>From: kkeller@mail.sas.upenn.edu (Keith Keller...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11309</td>\n",
       "      <td>From: adams@bellini.berkeley.edu (Adam L. Schw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11310</td>\n",
       "      <td>From: levin@bbn.com (Joel B Levin)\\nSubject: R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11311</td>\n",
       "      <td>From: tedward@cs.cornell.edu (Edward [Ted] Fis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11312</td>\n",
       "      <td>From: mori@volga.mfd.cs.fujitsu.co.jp (Tsuyosh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11313</td>\n",
       "      <td>From: marc@yogi.austin.ibm.com (Marc J. Stephe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document\n",
       "0      From: ab4z@Virginia.EDU (\"Andi Beyer\")\\nSubjec...\n",
       "1      From: timmbake@mcl.ucsb.edu (Bake Timmons)\\nSu...\n",
       "2      From: bc744@cleveland.Freenet.Edu (Mark Ira Ka...\n",
       "3      From: ray@ole.cdac.com (Ray Berry)\\nSubject: C...\n",
       "4      From: kkeller@mail.sas.upenn.edu (Keith Keller...\n",
       "...                                                  ...\n",
       "11309  From: adams@bellini.berkeley.edu (Adam L. Schw...\n",
       "11310  From: levin@bbn.com (Joel B Levin)\\nSubject: R...\n",
       "11311  From: tedward@cs.cornell.edu (Edward [Ted] Fis...\n",
       "11312  From: mori@volga.mfd.cs.fujitsu.co.jp (Tsuyosh...\n",
       "11313  From: marc@yogi.austin.ibm.com (Marc J. Stephe...\n",
       "\n",
       "[11314 rows x 1 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "type(documents)\n",
    "newsDf = pd.DataFrame({\"document\":documents})\n",
    "newsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#특수문자 제거(영문자를 제외)\n",
    "newsDf['clean_doc'] = newsDf['document'].str.replace(\"[^a-zA-Z]\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>clean_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>From: ab4z@Virginia.EDU (\"Andi Beyer\")\\nSubjec...</td>\n",
       "      <td>From  ab z Virginia EDU   Andi Beyer   Subject...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>From: timmbake@mcl.ucsb.edu (Bake Timmons)\\nSu...</td>\n",
       "      <td>From  timmbake mcl ucsb edu  Bake Timmons  Sub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>From: bc744@cleveland.Freenet.Edu (Mark Ira Ka...</td>\n",
       "      <td>From  bc    cleveland Freenet Edu  Mark Ira Ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>From: ray@ole.cdac.com (Ray Berry)\\nSubject: C...</td>\n",
       "      <td>From  ray ole cdac com  Ray Berry  Subject  Cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>From: kkeller@mail.sas.upenn.edu (Keith Keller...</td>\n",
       "      <td>From  kkeller mail sas upenn edu  Keith Keller...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11309</td>\n",
       "      <td>From: adams@bellini.berkeley.edu (Adam L. Schw...</td>\n",
       "      <td>From  adams bellini berkeley edu  Adam L  Schw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11310</td>\n",
       "      <td>From: levin@bbn.com (Joel B Levin)\\nSubject: R...</td>\n",
       "      <td>From  levin bbn com  Joel B Levin  Subject  Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11311</td>\n",
       "      <td>From: tedward@cs.cornell.edu (Edward [Ted] Fis...</td>\n",
       "      <td>From  tedward cs cornell edu  Edward  Ted  Fis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11312</td>\n",
       "      <td>From: mori@volga.mfd.cs.fujitsu.co.jp (Tsuyosh...</td>\n",
       "      <td>From  mori volga mfd cs fujitsu co jp  Tsuyosh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11313</td>\n",
       "      <td>From: marc@yogi.austin.ibm.com (Marc J. Stephe...</td>\n",
       "      <td>From  marc yogi austin ibm com  Marc J  Stephe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document                                          clean_doc\n",
       "0      From: ab4z@Virginia.EDU (\"Andi Beyer\")\\nSubjec...  From  ab z Virginia EDU   Andi Beyer   Subject...\n",
       "1      From: timmbake@mcl.ucsb.edu (Bake Timmons)\\nSu...  From  timmbake mcl ucsb edu  Bake Timmons  Sub...\n",
       "2      From: bc744@cleveland.Freenet.Edu (Mark Ira Ka...  From  bc    cleveland Freenet Edu  Mark Ira Ka...\n",
       "3      From: ray@ole.cdac.com (Ray Berry)\\nSubject: C...  From  ray ole cdac com  Ray Berry  Subject  Cl...\n",
       "4      From: kkeller@mail.sas.upenn.edu (Keith Keller...  From  kkeller mail sas upenn edu  Keith Keller...\n",
       "...                                                  ...                                                ...\n",
       "11309  From: adams@bellini.berkeley.edu (Adam L. Schw...  From  adams bellini berkeley edu  Adam L  Schw...\n",
       "11310  From: levin@bbn.com (Joel B Levin)\\nSubject: R...  From  levin bbn com  Joel B Levin  Subject  Re...\n",
       "11311  From: tedward@cs.cornell.edu (Edward [Ted] Fis...  From  tedward cs cornell edu  Edward  Ted  Fis...\n",
       "11312  From: mori@volga.mfd.cs.fujitsu.co.jp (Tsuyosh...  From  mori volga mfd cs fujitsu co jp  Tsuyosh...\n",
       "11313  From: marc@yogi.austin.ibm.com (Marc J. Stephe...  From  marc yogi austin ibm com  Marc J  Stephe...\n",
       "\n",
       "[11314 rows x 2 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3글자 이하 단어 제거, 소문자 변환\n",
    "newsDf['clean_doc'] = newsDf['clean_doc'].apply(lambda x:' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#소문자 변환\n",
    "newsDf['clean_doc'] = newsDf['clean_doc'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        from virginia andi beyer subject israeli terro...\n",
       "1        from timmbake ucsb bake timmons subject amusin...\n",
       "2        from cleveland freenet mark kaufman subject re...\n",
       "3        from cdac berry subject clipper business usual...\n",
       "4        from kkeller mail upenn keith keller subject p...\n",
       "                               ...                        \n",
       "11309    from adams bellini berkeley adam schwartz subj...\n",
       "11310    from levin joel levin subject selective placeb...\n",
       "11311    from tedward cornell edward fischer subject be...\n",
       "11312    from mori volga fujitsu tsuyoshi mori subject ...\n",
       "11313    from marc yogi austin marc stephenson subject ...\n",
       "Name: clean_doc, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsDf['clean_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 제거\n",
    "sw = stopwords.words('english')\n",
    "#토큰화\n",
    "tokenizedDoc = newsDf['clean_doc'].apply(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizedDoc = tokenizedDoc.apply(lambda x: [item for item in x if item not in sw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['timmbake',\n",
       " 'ucsb',\n",
       " 'bake',\n",
       " 'timmons',\n",
       " 'subject',\n",
       " 'amusing',\n",
       " 'atheists',\n",
       " 'agnostics',\n",
       " 'lines',\n",
       " 'yeah',\n",
       " 'expect',\n",
       " 'people',\n",
       " 'read',\n",
       " 'actually',\n",
       " 'accept',\n",
       " 'hard',\n",
       " 'atheism',\n",
       " 'need',\n",
       " 'little',\n",
       " 'leap',\n",
       " 'faith',\n",
       " 'jimmy',\n",
       " 'logic',\n",
       " 'runs',\n",
       " 'steam',\n",
       " 'sorry',\n",
       " 'pity',\n",
       " 'sorry',\n",
       " 'feelings',\n",
       " 'denial',\n",
       " 'faith',\n",
       " 'need',\n",
       " 'well',\n",
       " 'pretend',\n",
       " 'happily',\n",
       " 'ever',\n",
       " 'anyway',\n",
       " 'maybe',\n",
       " 'start',\n",
       " 'newsgroup',\n",
       " 'atheist',\n",
       " 'hard',\n",
       " 'bummin',\n",
       " 'much',\n",
       " 'forget',\n",
       " 'flintstone',\n",
       " 'chewables',\n",
       " 'bake',\n",
       " 'timmons']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizedDoc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from timmbake ucsb bake timmons subject amusing atheists agnostics lines yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-DF 매트릭스 구성\n",
    "#TFIDF는 토큰화가 안되어 있는 텍스트 데이터로 구성\n",
    "#토큰화 <-> 역토큰화(토큰화 취소)\n",
    "newsDf['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#역토큰화\n",
    "deTokenizedDoc=[]\n",
    "for i in range(len(newsDf)):\n",
    "    temp = ' '.join(tokenizedDoc[i])\n",
    "    deTokenizedDoc.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsDf['clean_doc'] = deTokenizedDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'timmbake ucsb bake timmons subject amusing atheists agnostics lines yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgroup atheist hard bummin much forget flintstone chewables bake timmons'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsDf['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf 행렬 구성\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vector = TfidfVectorizer(stop_words='english',max_features=1000) #1000개 단어\n",
    "res = vector.fit_transform(newsDf['clean_doc'])\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 332684 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svd-full, truncated \n",
    "#특이값분해\n",
    "#행렬= U*S*VT\n",
    "#절단된 SVD -> 차원 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#토픽 숫자: n_components\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svdModel = TruncatedSVD(n_components=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=20, n_iter=5,\n",
       "             random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svdModel.fit(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(svdModel.components_) #VT\n",
    "#20개의 토픽과 1000개의 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ability',\n",
       " 'able',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'according',\n",
       " 'account',\n",
       " 'action',\n",
       " 'actually',\n",
       " 'added',\n",
       " 'addition',\n",
       " 'address',\n",
       " 'administration',\n",
       " 'advance',\n",
       " 'advice',\n",
       " 'agencies',\n",
       " 'agree',\n",
       " 'algorithm',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allows',\n",
       " 'amendment',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'analysis',\n",
       " 'andrew',\n",
       " 'angeles',\n",
       " 'anonymous',\n",
       " 'answer',\n",
       " 'answers',\n",
       " 'anti',\n",
       " 'anybody',\n",
       " 'apparently',\n",
       " 'appear',\n",
       " 'appears',\n",
       " 'apple',\n",
       " 'application',\n",
       " 'applications',\n",
       " 'apply',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'approach',\n",
       " 'appropriate',\n",
       " 'april',\n",
       " 'arab',\n",
       " 'archive',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'argic',\n",
       " 'argument',\n",
       " 'arguments',\n",
       " 'armenia',\n",
       " 'armenian',\n",
       " 'armenians',\n",
       " 'arms',\n",
       " 'army',\n",
       " 'article',\n",
       " 'articles',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'assume',\n",
       " 'atheism',\n",
       " 'atheists',\n",
       " 'attack',\n",
       " 'attempt',\n",
       " 'austin',\n",
       " 'australia',\n",
       " 'author',\n",
       " 'authority',\n",
       " 'available',\n",
       " 'average',\n",
       " 'avoid',\n",
       " 'away',\n",
       " 'banks',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basis',\n",
       " 'belief',\n",
       " 'believe',\n",
       " 'bell',\n",
       " 'berkeley',\n",
       " 'best',\n",
       " 'better',\n",
       " 'bible',\n",
       " 'bike',\n",
       " 'billion',\n",
       " 'bios',\n",
       " 'bits',\n",
       " 'black',\n",
       " 'block',\n",
       " 'blood',\n",
       " 'blue',\n",
       " 'board',\n",
       " 'body',\n",
       " 'book',\n",
       " 'books',\n",
       " 'boston',\n",
       " 'bought',\n",
       " 'break',\n",
       " 'brian',\n",
       " 'bring',\n",
       " 'british',\n",
       " 'brought',\n",
       " 'brown',\n",
       " 'buffalo',\n",
       " 'build',\n",
       " 'building',\n",
       " 'built',\n",
       " 'business',\n",
       " 'cable',\n",
       " 'california',\n",
       " 'called',\n",
       " 'calls',\n",
       " 'caltech',\n",
       " 'came',\n",
       " 'canada',\n",
       " 'card',\n",
       " 'cards',\n",
       " 'care',\n",
       " 'carry',\n",
       " 'cars',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'cause',\n",
       " 'center',\n",
       " 'central',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changes',\n",
       " 'cheap',\n",
       " 'check',\n",
       " 'chicago',\n",
       " 'child',\n",
       " 'children',\n",
       " 'chip',\n",
       " 'chips',\n",
       " 'choice',\n",
       " 'chris',\n",
       " 'christ',\n",
       " 'christian',\n",
       " 'christianity',\n",
       " 'christians',\n",
       " 'church',\n",
       " 'citizens',\n",
       " 'city',\n",
       " 'civil',\n",
       " 'claim',\n",
       " 'claims',\n",
       " 'class',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'cleveland',\n",
       " 'clinton',\n",
       " 'clipper',\n",
       " 'clock',\n",
       " 'close',\n",
       " 'code',\n",
       " 'college',\n",
       " 'color',\n",
       " 'colorado',\n",
       " 'colors',\n",
       " 'columbia',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'command',\n",
       " 'comment',\n",
       " 'comments',\n",
       " 'commercial',\n",
       " 'committee',\n",
       " 'common',\n",
       " 'communication',\n",
       " 'communications',\n",
       " 'community',\n",
       " 'comp',\n",
       " 'companies',\n",
       " 'company',\n",
       " 'complete',\n",
       " 'completely',\n",
       " 'computer',\n",
       " 'computing',\n",
       " 'condition',\n",
       " 'conference',\n",
       " 'congress',\n",
       " 'consider',\n",
       " 'considered',\n",
       " 'contact',\n",
       " 'contains',\n",
       " 'context',\n",
       " 'continue',\n",
       " 'control',\n",
       " 'controller',\n",
       " 'copy',\n",
       " 'corp',\n",
       " 'corporation',\n",
       " 'correct',\n",
       " 'cost',\n",
       " 'costs',\n",
       " 'count',\n",
       " 'countries',\n",
       " 'country',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'court',\n",
       " 'cover',\n",
       " 'create',\n",
       " 'created',\n",
       " 'crime',\n",
       " 'cross',\n",
       " 'crypto',\n",
       " 'cryptography',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'cwru',\n",
       " 'data',\n",
       " 'date',\n",
       " 'dave',\n",
       " 'david',\n",
       " 'days',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'death',\n",
       " 'decided',\n",
       " 'defense',\n",
       " 'define',\n",
       " 'deleted',\n",
       " 'department',\n",
       " 'dept',\n",
       " 'design',\n",
       " 'designed',\n",
       " 'details',\n",
       " 'detroit',\n",
       " 'developed',\n",
       " 'development',\n",
       " 'device',\n",
       " 'devices',\n",
       " 'died',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'digex',\n",
       " 'digital',\n",
       " 'directly',\n",
       " 'directory',\n",
       " 'disclaimer',\n",
       " 'discussion',\n",
       " 'disease',\n",
       " 'disk',\n",
       " 'disks',\n",
       " 'display',\n",
       " 'distribution',\n",
       " 'division',\n",
       " 'doctor',\n",
       " 'door',\n",
       " 'double',\n",
       " 'doubt',\n",
       " 'doug',\n",
       " 'drive',\n",
       " 'driver',\n",
       " 'drivers',\n",
       " 'drives',\n",
       " 'drug',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'earth',\n",
       " 'easily',\n",
       " 'east',\n",
       " 'easy',\n",
       " 'education',\n",
       " 'effect',\n",
       " 'effective',\n",
       " 'electronic',\n",
       " 'email',\n",
       " 'encryption',\n",
       " 'enforcement',\n",
       " 'engine',\n",
       " 'engineering',\n",
       " 'entire',\n",
       " 'entry',\n",
       " 'environment',\n",
       " 'equipment',\n",
       " 'eric',\n",
       " 'error',\n",
       " 'escrow',\n",
       " 'especially',\n",
       " 'europe',\n",
       " 'event',\n",
       " 'events',\n",
       " 'evidence',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'excellent',\n",
       " 'exist',\n",
       " 'existence',\n",
       " 'exists',\n",
       " 'expect',\n",
       " 'expected',\n",
       " 'experience',\n",
       " 'explain',\n",
       " 'export',\n",
       " 'express',\n",
       " 'extra',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'faith',\n",
       " 'fall',\n",
       " 'false',\n",
       " 'family',\n",
       " 'fast',\n",
       " 'faster',\n",
       " 'father',\n",
       " 'features',\n",
       " 'federal',\n",
       " 'feel',\n",
       " 'field',\n",
       " 'figure',\n",
       " 'file',\n",
       " 'files',\n",
       " 'final',\n",
       " 'finally',\n",
       " 'fine',\n",
       " 'firearms',\n",
       " 'floppy',\n",
       " 'folks',\n",
       " 'follow',\n",
       " 'following',\n",
       " 'font',\n",
       " 'fonts',\n",
       " 'food',\n",
       " 'force',\n",
       " 'form',\n",
       " 'format',\n",
       " 'frank',\n",
       " 'free',\n",
       " 'freedom',\n",
       " 'freenet',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'function',\n",
       " 'functions',\n",
       " 'future',\n",
       " 'game',\n",
       " 'games',\n",
       " 'gary',\n",
       " 'gave',\n",
       " 'general',\n",
       " 'generally',\n",
       " 'genocide',\n",
       " 'george',\n",
       " 'germany',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'giving',\n",
       " 'goal',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'gordon',\n",
       " 'government',\n",
       " 'graphics',\n",
       " 'great',\n",
       " 'greek',\n",
       " 'ground',\n",
       " 'group',\n",
       " 'groups',\n",
       " 'guess',\n",
       " 'guns',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happens',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hardware',\n",
       " 'harvard',\n",
       " 'head',\n",
       " 'health',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'held',\n",
       " 'hell',\n",
       " 'help',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'history',\n",
       " 'hockey',\n",
       " 'hold',\n",
       " 'holy',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'host',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'human',\n",
       " 'idea',\n",
       " 'ideas',\n",
       " 'illinois',\n",
       " 'image',\n",
       " 'images',\n",
       " 'imagine',\n",
       " 'important',\n",
       " 'include',\n",
       " 'included',\n",
       " 'includes',\n",
       " 'including',\n",
       " 'increase',\n",
       " 'indiana',\n",
       " 'individual',\n",
       " 'info',\n",
       " 'information',\n",
       " 'input',\n",
       " 'inside',\n",
       " 'installed',\n",
       " 'instead',\n",
       " 'institute',\n",
       " 'insurance',\n",
       " 'intended',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'interface',\n",
       " 'internal',\n",
       " 'international',\n",
       " 'internet',\n",
       " 'involved',\n",
       " 'islam',\n",
       " 'israel',\n",
       " 'israeli',\n",
       " 'issue',\n",
       " 'issues',\n",
       " 'james',\n",
       " 'jesus',\n",
       " 'jewish',\n",
       " 'jews',\n",
       " 'jobs',\n",
       " 'john',\n",
       " 'jpeg',\n",
       " 'keith',\n",
       " 'kept',\n",
       " 'keyboard',\n",
       " 'keys',\n",
       " 'keywords',\n",
       " 'kill',\n",
       " 'killed',\n",
       " 'kind',\n",
       " 'king',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'knowledge',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'koresh',\n",
       " 'laboratory',\n",
       " 'lack',\n",
       " 'land',\n",
       " 'language',\n",
       " 'large',\n",
       " 'late',\n",
       " 'later',\n",
       " 'launch',\n",
       " 'laws',\n",
       " 'lead',\n",
       " 'league',\n",
       " 'learn',\n",
       " 'leave',\n",
       " 'left',\n",
       " 'legal',\n",
       " 'letter',\n",
       " 'level',\n",
       " 'library',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'limited',\n",
       " 'line',\n",
       " 'lines',\n",
       " 'list',\n",
       " 'little',\n",
       " 'live',\n",
       " 'lives',\n",
       " 'living',\n",
       " 'local',\n",
       " 'logic',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'looked',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'lord',\n",
       " 'lost',\n",
       " 'lots',\n",
       " 'love',\n",
       " 'lower',\n",
       " 'lunar',\n",
       " 'machine',\n",
       " 'machines',\n",
       " 'magazine',\n",
       " 'mail',\n",
       " 'mailing',\n",
       " 'main',\n",
       " 'major',\n",
       " 'majority',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'management',\n",
       " 'manager',\n",
       " 'manual',\n",
       " 'march',\n",
       " 'mark',\n",
       " 'market',\n",
       " 'mass',\n",
       " 'master',\n",
       " 'material',\n",
       " 'math',\n",
       " 'matter',\n",
       " 'matthew',\n",
       " 'maybe',\n",
       " 'mean',\n",
       " 'meaning',\n",
       " 'means',\n",
       " 'media',\n",
       " 'medical',\n",
       " 'medicine',\n",
       " 'member',\n",
       " 'members',\n",
       " 'memory',\n",
       " 'mention',\n",
       " 'mentioned',\n",
       " 'message',\n",
       " 'method',\n",
       " 'michael',\n",
       " 'microsoft',\n",
       " 'middle',\n",
       " 'mike',\n",
       " 'miles',\n",
       " 'military',\n",
       " 'million',\n",
       " 'mind',\n",
       " 'minutes',\n",
       " 'misc',\n",
       " 'mission',\n",
       " 'mode',\n",
       " 'model',\n",
       " 'modem',\n",
       " 'modern',\n",
       " 'money',\n",
       " 'monitor',\n",
       " 'month',\n",
       " 'months',\n",
       " 'moon',\n",
       " 'moral',\n",
       " 'morality',\n",
       " 'mother',\n",
       " 'motherboard',\n",
       " 'motif',\n",
       " 'mouse',\n",
       " 'multi',\n",
       " 'multiple',\n",
       " 'muslim',\n",
       " 'names',\n",
       " 'nasa',\n",
       " 'national',\n",
       " 'natural',\n",
       " 'nature',\n",
       " 'navy',\n",
       " 'near',\n",
       " 'necessarily',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needs',\n",
       " 'netcom',\n",
       " 'network',\n",
       " 'news',\n",
       " 'newsgroup',\n",
       " 'newsreader',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'nntp',\n",
       " 'normal',\n",
       " 'north',\n",
       " 'note',\n",
       " 'nuclear',\n",
       " 'null',\n",
       " 'number',\n",
       " 'numbers',\n",
       " 'object',\n",
       " 'objective',\n",
       " 'obvious',\n",
       " 'obviously',\n",
       " 'offer',\n",
       " 'office',\n",
       " 'official',\n",
       " 'ohio',\n",
       " 'ones',\n",
       " 'online',\n",
       " 'open',\n",
       " 'operation',\n",
       " 'opinion',\n",
       " 'opinions',\n",
       " 'option',\n",
       " 'options',\n",
       " 'orbit',\n",
       " 'order',\n",
       " 'organization',\n",
       " 'original',\n",
       " 'originator',\n",
       " 'output',\n",
       " 'outside',\n",
       " 'package',\n",
       " 'page',\n",
       " 'pain',\n",
       " 'paper',\n",
       " 'particular',\n",
       " 'parts',\n",
       " 'party',\n",
       " 'pass',\n",
       " 'past',\n",
       " 'paul',\n",
       " 'peace',\n",
       " 'people',\n",
       " 'perfect',\n",
       " 'performance',\n",
       " 'period',\n",
       " 'person',\n",
       " 'personal',\n",
       " 'peter',\n",
       " 'phone',\n",
       " 'physical',\n",
       " 'physics',\n",
       " 'pick',\n",
       " 'picture',\n",
       " 'pitt',\n",
       " 'pittsburgh',\n",
       " 'place',\n",
       " 'places',\n",
       " 'plan',\n",
       " 'play',\n",
       " 'played',\n",
       " 'player',\n",
       " 'players',\n",
       " 'plus',\n",
       " 'point',\n",
       " 'points',\n",
       " 'police',\n",
       " 'policy',\n",
       " 'political',\n",
       " 'poor',\n",
       " 'population',\n",
       " 'port',\n",
       " 'position',\n",
       " 'possible',\n",
       " 'possibly',\n",
       " 'post',\n",
       " 'posted',\n",
       " 'posting',\n",
       " 'posts',\n",
       " 'postscript',\n",
       " 'power',\n",
       " 'present',\n",
       " 'president',\n",
       " 'press',\n",
       " 'pretty',\n",
       " 'prevent',\n",
       " 'previous',\n",
       " 'price',\n",
       " 'printer',\n",
       " 'privacy',\n",
       " 'private',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'problems',\n",
       " 'process',\n",
       " 'processing',\n",
       " 'product',\n",
       " 'products',\n",
       " 'program',\n",
       " 'programming',\n",
       " 'programs',\n",
       " 'project',\n",
       " 'protect',\n",
       " 'prove',\n",
       " 'provide',\n",
       " 'provided',\n",
       " 'provides',\n",
       " 'public',\n",
       " 'published',\n",
       " 'purdue',\n",
       " 'purpose',\n",
       " 'quality',\n",
       " 'question',\n",
       " 'questions',\n",
       " 'quite',\n",
       " 'quote',\n",
       " 'radio',\n",
       " 'random',\n",
       " 'range',\n",
       " 'rate',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'real',\n",
       " 'reality',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'reasonable',\n",
       " 'reasons',\n",
       " 'received',\n",
       " 'recent',\n",
       " 'recently',\n",
       " 'record',\n",
       " 'reference',\n",
       " 'references',\n",
       " 'regarding',\n",
       " 'regular',\n",
       " 'related',\n",
       " 'release',\n",
       " 'religion',\n",
       " 'religious',\n",
       " 'remember',\n",
       " 'remote',\n",
       " 'reply',\n",
       " 'report',\n",
       " 'reported',\n",
       " 'reports',\n",
       " 'request',\n",
       " 'require',\n",
       " 'required',\n",
       " 'requires',\n",
       " 'research',\n",
       " 'reserve',\n",
       " 'resource',\n",
       " 'resources',\n",
       " 'response',\n",
       " 'rest',\n",
       " 'result',\n",
       " 'results',\n",
       " 'return',\n",
       " 'richard',\n",
       " 'right',\n",
       " 'rights',\n",
       " 'risk',\n",
       " 'road',\n",
       " 'robert',\n",
       " 'rochester',\n",
       " 'roger',\n",
       " 'room',\n",
       " 'round',\n",
       " 'rule',\n",
       " 'rules',\n",
       " 'running',\n",
       " 'runs',\n",
       " 'russian',\n",
       " 'safety',\n",
       " 'said',\n",
       " 'sale',\n",
       " 'satellite',\n",
       " 'save',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'school',\n",
       " 'science',\n",
       " 'scientific',\n",
       " 'scott',\n",
       " 'screen',\n",
       " 'scsi',\n",
       " 'search',\n",
       " 'season',\n",
       " 'second',\n",
       " 'secret',\n",
       " 'section',\n",
       " 'secure',\n",
       " 'security',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'sell',\n",
       " 'send',\n",
       " 'sense',\n",
       " 'sent',\n",
       " 'serdar',\n",
       " 'serial',\n",
       " 'series',\n",
       " 'server',\n",
       " 'service',\n",
       " 'services',\n",
       " 'shall',\n",
       " 'shipping',\n",
       " 'short',\n",
       " 'shot',\n",
       " 'shows',\n",
       " 'shuttle',\n",
       " 'signal',\n",
       " 'similar',\n",
       " 'simple',\n",
       " 'simply',\n",
       " 'single',\n",
       " 'site',\n",
       " 'sites',\n",
       " 'situation',\n",
       " 'size',\n",
       " 'small',\n",
       " 'smith',\n",
       " 'society',\n",
       " 'software',\n",
       " 'soldiers',\n",
       " 'solution',\n",
       " 'somebody',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'sort',\n",
       " 'sound',\n",
       " 'sounds',\n",
       " 'source',\n",
       " 'sources',\n",
       " 'south',\n",
       " 'soviet',\n",
       " 'space',\n",
       " 'speak',\n",
       " 'special',\n",
       " 'specific',\n",
       " 'speed',\n",
       " 'spirit',\n",
       " 'stand',\n",
       " 'standard',\n",
       " 'standards',\n",
       " 'stanford',\n",
       " 'start',\n",
       " 'started',\n",
       " 'state',\n",
       " 'stated',\n",
       " 'statement',\n",
       " 'states',\n",
       " 'station',\n",
       " 'stay',\n",
       " 'step',\n",
       " 'stephanopoulos',\n",
       " 'steve',\n",
       " 'stop',\n",
       " 'story',\n",
       " 'stream',\n",
       " 'street',\n",
       " 'strong',\n",
       " 'student',\n",
       " 'studies',\n",
       " 'study',\n",
       " 'stuff',\n",
       " 'subject',\n",
       " 'suggest',\n",
       " 'suggestions',\n",
       " 'summary',\n",
       " 'summer',\n",
       " 'supply',\n",
       " 'support',\n",
       " 'supports',\n",
       " 'supposed',\n",
       " 'sure',\n",
       " 'surface',\n",
       " 'suspect',\n",
       " 'switch',\n",
       " 'systems',\n",
       " 'table',\n",
       " 'taken',\n",
       " 'takes',\n",
       " 'taking',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'tape',\n",
       " 'team',\n",
       " 'teams',\n",
       " 'technical',\n",
       " 'technology',\n",
       " 'tell',\n",
       " 'term',\n",
       " 'terms',\n",
       " 'test',\n",
       " 'texas',\n",
       " 'text',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'theory',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinking',\n",
       " 'thomas',\n",
       " 'thought',\n",
       " 'time',\n",
       " 'times',\n",
       " 'title',\n",
       " 'today',\n",
       " 'told',\n",
       " 'took',\n",
       " 'tools',\n",
       " 'toronto',\n",
       " 'total',\n",
       " 'town',\n",
       " 'trade',\n",
       " 'traffic',\n",
       " 'transfer',\n",
       " 'tried',\n",
       " 'trouble',\n",
       " 'true',\n",
       " 'trust',\n",
       " 'truth',\n",
       " 'trying',\n",
       " 'turkey',\n",
       " 'turkish',\n",
       " 'turks',\n",
       " 'turn',\n",
       " 'turned',\n",
       " 'type',\n",
       " 'types',\n",
       " 'uiuc',\n",
       " 'understand',\n",
       " 'understanding',\n",
       " 'unfortunately',\n",
       " 'unit',\n",
       " 'united',\n",
       " 'univ',\n",
       " 'university',\n",
       " 'unix',\n",
       " 'unless',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'usenet',\n",
       " 'user',\n",
       " 'users',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'utexas',\n",
       " 'uucp',\n",
       " 'value',\n",
       " 'values',\n",
       " 'vancouver',\n",
       " 'various',\n",
       " 'version',\n",
       " 'video',\n",
       " 'view',\n",
       " 'views',\n",
       " 'virginia',\n",
       " 'voice',\n",
       " 'volume',\n",
       " 'wait',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'wants',\n",
       " 'washington',\n",
       " 'watch',\n",
       " 'water',\n",
       " 'ways',\n",
       " 'weapons',\n",
       " 'week',\n",
       " 'weeks',\n",
       " 'went',\n",
       " 'west',\n",
       " 'western',\n",
       " 'white',\n",
       " 'wide',\n",
       " 'widget',\n",
       " 'wife',\n",
       " 'willing',\n",
       " 'window',\n",
       " 'windows',\n",
       " 'wire',\n",
       " 'wish',\n",
       " 'woman',\n",
       " 'women',\n",
       " 'wonder',\n",
       " 'word',\n",
       " 'words',\n",
       " 'work',\n",
       " 'worked',\n",
       " 'working',\n",
       " 'works',\n",
       " 'world',\n",
       " 'worse',\n",
       " 'worth',\n",
       " 'write',\n",
       " 'writing',\n",
       " 'written',\n",
       " 'wrong',\n",
       " 'wrote',\n",
       " 'xterm',\n",
       " 'year',\n",
       " 'years',\n",
       " 'york',\n",
       " 'young']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = vector.get_feature_names() #1000개 단어\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토픽 1: [('subject', 0.265), ('lines', 0.26495), ('organization', 0.26304), ('posting', 0.24983), ('nntp', 0.24466), ('host', 0.24465), ('university', 0.21389), ('distribution', 0.15693), ('reply', 0.12507), ('like', 0.12268)]\n",
      "토픽 2: [('nntp', 0.3406), ('host', 0.33811), ('posting', 0.33318), ('university', 0.15258), ('organization', 0.12408), ('lines', 0.12285), ('subject', 0.12), ('distribution', 0.11668), ('reply', 0.05804), ('cwru', 0.04312)]\n",
      "토픽 3: [('windows', 0.39288), ('card', 0.18356), ('file', 0.16128), ('thanks', 0.15623), ('drive', 0.15234), ('help', 0.12667), ('files', 0.12367), ('video', 0.11941), ('version', 0.11449), ('window', 0.11145)]\n",
      "토픽 4: [('university', 0.42243), ('state', 0.2169), ('pitt', 0.20051), ('virginia', 0.19311), ('organization', 0.16167), ('lines', 0.1611), ('subject', 0.16071), ('gordon', 0.15877), ('banks', 0.15715), ('computer', 0.12301)]\n",
      "토픽 5: [('pitt', 0.31337), ('gordon', 0.27551), ('banks', 0.26785), ('nasa', 0.16989), ('distribution', 0.16293), ('science', 0.14182), ('computer', 0.14059), ('reply', 0.1371), ('world', 0.13431), ('pittsburgh', 0.12395)]\n",
      "토픽 6: [('pitt', 0.3595), ('gordon', 0.31599), ('banks', 0.31051), ('cleveland', 0.25518), ('cwru', 0.24503), ('freenet', 0.16532), ('pittsburgh', 0.15784), ('western', 0.14181), ('reserve', 0.14018), ('computer', 0.14011)]\n",
      "토픽 7: [('access', 0.24836), ('cleveland', 0.23935), ('nasa', 0.23778), ('state', 0.21442), ('ohio', 0.21052), ('cwru', 0.19804), ('space', 0.15717), ('freenet', 0.15488), ('clipper', 0.15271), ('public', 0.13173)]\n",
      "토픽 8: [('nasa', 0.5737), ('space', 0.35351), ('cleveland', 0.16081), ('team', 0.16064), ('game', 0.15418), ('year', 0.13929), ('center', 0.13608), ('cwru', 0.12421), ('games', 0.11005), ('research', 0.10237)]\n",
      "토픽 9: [('access', 0.31036), ('drive', 0.23691), ('sale', 0.22787), ('digex', 0.18509), ('chip', 0.15121), ('clipper', 0.14589), ('game', 0.13779), ('team', 0.12748), ('netcom', 0.12375), ('communications', 0.11852)]\n",
      "토픽 10: [('state', 0.41379), ('ohio', 0.2476), ('drive', 0.20261), ('nasa', 0.17593), ('scsi', 0.13837), ('computer', 0.12249), ('sale', 0.10041), ('chip', 0.09989), ('clipper', 0.08612), ('science', 0.083)]\n",
      "토픽 11: [('state', 0.37785), ('ohio', 0.25164), ('access', 0.24652), ('windows', 0.22585), ('digex', 0.17268), ('team', 0.15077), ('game', 0.13216), ('file', 0.10362), ('year', 0.1016), ('files', 0.08888)]\n",
      "토픽 12: [('clipper', 0.32105), ('chip', 0.29621), ('encryption', 0.21718), ('netcom', 0.16201), ('escrow', 0.13393), ('reply', 0.13097), ('keys', 0.10894), ('algorithm', 0.1018), ('technology', 0.09655), ('government', 0.08964)]\n",
      "토픽 13: [('virginia', 0.47042), ('caltech', 0.16475), ('space', 0.16129), ('drive', 0.15394), ('technology', 0.15167), ('nasa', 0.15121), ('university', 0.1426), ('institute', 0.13132), ('keith', 0.11912), ('scsi', 0.08701)]\n",
      "토픽 14: [('caltech', 0.39443), ('institute', 0.32119), ('technology', 0.28991), ('sale', 0.28064), ('keith', 0.27566), ('california', 0.19854), ('netcom', 0.19554), ('atheists', 0.11853), ('windows', 0.09758), ('state', 0.09421)]\n",
      "토픽 15: [('netcom', 0.4923), ('services', 0.20973), ('communication', 0.12405), ('online', 0.11963), ('space', 0.1184), ('david', 0.10851), ('bike', 0.10661), ('college', 0.10285), ('line', 0.094), ('newsreader', 0.08427)]\n",
      "토픽 16: [('virginia', 0.39909), ('sale', 0.37007), ('mail', 0.12396), ('bike', 0.11195), ('price', 0.10777), ('good', 0.10576), ('offer', 0.10277), ('condition', 0.09648), ('shipping', 0.08011), ('like', 0.07984)]\n",
      "토픽 17: [('virginia', 0.50174), ('netcom', 0.342), ('windows', 0.19951), ('distribution', 0.16382), ('world', 0.1439), ('services', 0.1265), ('drive', 0.12112), ('pitt', 0.10895), ('state', 0.09813), ('gordon', 0.09674)]\n",
      "토픽 18: [('card', 0.43229), ('uiuc', 0.26562), ('illinois', 0.22419), ('video', 0.21804), ('article', 0.17948), ('drivers', 0.14864), ('people', 0.11798), ('cards', 0.10694), ('university', 0.10618), ('news', 0.10589)]\n",
      "토픽 19: [('sale', 0.32538), ('windows', 0.22041), ('uiuc', 0.21031), ('colorado', 0.2043), ('illinois', 0.18242), ('berkeley', 0.17928), ('space', 0.15437), ('file', 0.15381), ('article', 0.13913), ('washington', 0.13661)]\n",
      "토픽 20: [('computer', 0.37763), ('science', 0.24351), ('netcom', 0.22488), ('distribution', 0.15413), ('department', 0.13695), ('virginia', 0.13134), ('like', 0.12691), ('services', 0.10753), ('world', 0.10337), ('purdue', 0.09542)]\n"
     ]
    }
   ],
   "source": [
    "def getTopic(c, fName, n=10):\n",
    "    for i, t in enumerate(c):\n",
    "        print(\"토픽 %d:\" %(i+1), [(fName[i],t[i].round(5)) for i in t.argsort()[:-n-1:-1]])\n",
    "    \n",
    "getTopic(svdModel.components_,terms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
